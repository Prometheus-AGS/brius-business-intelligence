# Quickstart: Constitutional Compliance Fixes

**Date**: 2025-01-18
**Feature**: Constitutional Compliance Fixes for Mastra Business Intelligence System
**Prerequisites**: Node.js 20+, Docker, pnpm

## Overview

This quickstart guide walks through implementing constitutional compliance fixes for the Mastra Business Intelligence System, addressing critical violations in database architecture, observability, MCP integration, and API validation.

## Prerequisites Validation

Before starting, verify your environment meets the constitutional requirements:

```bash
# Check Node.js version (requires 20+)
node --version

# Check Docker availability
docker --version

# Check pnpm availability
pnpm --version

# Verify git repository status
git status
```

## Phase 1: pgvector 17 Database Migration

### 1.1 Set Up pgvector 17 Database

**Create Docker environment for pgvector 17:**

```bash
# Create docker-compose.yml for pgvector 17
cat << 'EOF' > docker-compose.yml
version: '3.8'
services:
  postgres:
    image: pgvector/pgvector:pg17
    environment:
      POSTGRES_DB: mastra_bi
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: password
    ports:
      - "5432:5432"
    volumes:
      - pgdata:/var/lib/postgresql/data
      - ./migrations:/docker-entrypoint-initdb.d
    command: >
      postgres
      -c shared_buffers=1GB
      -c work_mem=256MB
      -c maintenance_work_mem=1GB
      -c effective_cache_size=3GB
      -c random_page_cost=1.1
      -c effective_io_concurrency=200
      -c max_connections=200

volumes:
  pgdata:
EOF

# Start pgvector database
docker-compose up -d postgres

# Verify pgvector extension is available
docker exec -it $(docker-compose ps -q postgres) psql -U postgres -d mastra_bi -c "CREATE EXTENSION IF NOT EXISTS vector;"
```

### 1.2 Install Database Dependencies

```bash
# Install pgvector and Drizzle dependencies
pnpm add pg @types/pg drizzle-orm drizzle-kit pgvector

# Install AWS Bedrock SDK for embeddings
pnpm add @aws-sdk/client-bedrock-runtime
```

### 1.3 Environment Configuration

```bash
# Update .env file with pgvector configuration
cat << 'EOF' >> .env
# pgvector Database (Constitutional Requirement)
PGVECTOR_DATABASE_URL=postgresql://postgres:password@localhost:5432/mastra_bi

# AWS Bedrock for embeddings (Constitutional Requirement)
AWS_REGION=us-east-1
AWS_ACCESS_KEY_ID=your_access_key
AWS_SECRET_ACCESS_KEY=your_secret_key

# Remove Supabase database references (Constitutional Violation)
# SUPABASE_URL=https://your-project.supabase.co
# SUPABASE_ANON_KEY=your-anon-key
EOF
```

### 1.4 Database Schema Setup

```bash
# Create migrations directory
mkdir -p migrations

# Create initial pgvector setup migration
cat << 'EOF' > migrations/001-setup-pgvector.sql
-- Enable pgvector extension
CREATE EXTENSION IF NOT EXISTS vector;

-- Create user_memories table
CREATE TABLE user_memories (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  user_id TEXT NOT NULL,
  content TEXT NOT NULL,
  embedding vector(1536) NOT NULL,
  category TEXT DEFAULT 'general',
  created_at TIMESTAMPTZ DEFAULT NOW(),
  updated_at TIMESTAMPTZ DEFAULT NOW(),
  metadata JSONB DEFAULT '{}'::jsonb,
  CONSTRAINT user_memories_embedding_not_null CHECK (embedding IS NOT NULL)
);

-- Create HNSW index for fast vector similarity search
CREATE INDEX CONCURRENTLY user_memories_embedding_hnsw_idx
ON user_memories
USING hnsw (embedding vector_l2_ops)
WITH (m = 16, ef_construction = 64);

-- Create additional indexes
CREATE INDEX user_memories_user_id_idx ON user_memories (user_id);
CREATE INDEX user_memories_category_idx ON user_memories (category);
CREATE INDEX user_memories_created_at_idx ON user_memories (created_at DESC);

-- Create global_memories table
CREATE TABLE global_memories (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  content TEXT NOT NULL,
  embedding vector(1536) NOT NULL,
  category TEXT DEFAULT 'general',
  access_level TEXT DEFAULT 'public',
  created_by TEXT,
  created_at TIMESTAMPTZ DEFAULT NOW(),
  updated_at TIMESTAMPTZ DEFAULT NOW(),
  metadata JSONB DEFAULT '{}'::jsonb,
  CONSTRAINT global_memories_embedding_not_null CHECK (embedding IS NOT NULL),
  CONSTRAINT global_memories_access_level_check CHECK (access_level IN ('public', 'restricted', 'admin'))
);

-- Create HNSW index for global memory search
CREATE INDEX CONCURRENTLY global_memories_embedding_hnsw_idx
ON global_memories
USING hnsw (embedding vector_l2_ops)
WITH (m = 16, ef_construction = 64);

-- Create knowledge_documents and document_chunks tables
CREATE TABLE knowledge_documents (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  title TEXT NOT NULL,
  content TEXT NOT NULL,
  file_path TEXT,
  file_type TEXT,
  file_size INTEGER,
  category TEXT DEFAULT 'general',
  tags TEXT[] DEFAULT '{}',
  upload_user_id TEXT,
  processing_status TEXT DEFAULT 'pending',
  created_at TIMESTAMPTZ DEFAULT NOW(),
  updated_at TIMESTAMPTZ DEFAULT NOW(),
  metadata JSONB DEFAULT '{}'::jsonb,
  CONSTRAINT knowledge_documents_processing_status_check
    CHECK (processing_status IN ('pending', 'processing', 'completed', 'failed'))
);

CREATE TABLE document_chunks (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  document_id UUID NOT NULL REFERENCES knowledge_documents(id) ON DELETE CASCADE,
  chunk_index INTEGER NOT NULL,
  content TEXT NOT NULL,
  embedding vector(1536) NOT NULL,
  chunk_metadata JSONB DEFAULT '{}'::jsonb,
  created_at TIMESTAMPTZ DEFAULT NOW(),
  CONSTRAINT document_chunks_embedding_not_null CHECK (embedding IS NOT NULL),
  CONSTRAINT document_chunks_chunk_index_positive CHECK (chunk_index >= 0),
  UNIQUE (document_id, chunk_index)
);

-- Create HNSW index for semantic search on document chunks
CREATE INDEX CONCURRENTLY document_chunks_embedding_hnsw_idx
ON document_chunks
USING hnsw (embedding vector_l2_ops)
WITH (m = 16, ef_construction = 64);

-- Migration status tracking
CREATE TABLE migration_status (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  migration_name TEXT NOT NULL UNIQUE,
  migration_type TEXT NOT NULL,
  status TEXT NOT NULL DEFAULT 'pending',
  started_at TIMESTAMPTZ,
  completed_at TIMESTAMPTZ,
  error_message TEXT,
  metadata JSONB DEFAULT '{}'::jsonb,
  CONSTRAINT migration_status_check CHECK (status IN ('pending', 'running', 'completed', 'failed'))
);
EOF

# Apply initial migration
docker exec -i $(docker-compose ps -q postgres) psql -U postgres -d mastra_bi < migrations/001-setup-pgvector.sql
```

### 1.5 Create Postgres Functions

```bash
# Create postgres functions for vector operations
cat << 'EOF' > migrations/002-create-functions.sql
-- Semantic search function
CREATE OR REPLACE FUNCTION semantic_search(
  query_embedding vector(1536),
  search_table TEXT DEFAULT 'user_memories',
  user_filter TEXT DEFAULT NULL,
  match_threshold float DEFAULT 0.8,
  match_count int DEFAULT 10
)
RETURNS TABLE (
  id UUID,
  content TEXT,
  similarity float,
  metadata JSONB
)
LANGUAGE plpgsql STABLE
AS $$
BEGIN
  IF search_table = 'user_memories' AND user_filter IS NOT NULL THEN
    RETURN QUERY
    SELECT
      um.id,
      um.content,
      1 - (um.embedding <=> query_embedding) as similarity,
      um.metadata
    FROM user_memories um
    WHERE um.user_id = user_filter
      AND 1 - (um.embedding <=> query_embedding) > match_threshold
    ORDER BY um.embedding <=> query_embedding
    LIMIT match_count;
  ELSIF search_table = 'global_memories' THEN
    RETURN QUERY
    SELECT
      gm.id,
      gm.content,
      1 - (gm.embedding <=> query_embedding) as similarity,
      gm.metadata
    FROM global_memories gm
    WHERE 1 - (gm.embedding <=> query_embedding) > match_threshold
    ORDER BY gm.embedding <=> query_embedding
    LIMIT match_count;
  ELSIF search_table = 'document_chunks' THEN
    RETURN QUERY
    SELECT
      dc.id,
      dc.content,
      1 - (dc.embedding <=> query_embedding) as similarity,
      dc.chunk_metadata as metadata
    FROM document_chunks dc
    WHERE 1 - (dc.embedding <=> query_embedding) > match_threshold
    ORDER BY dc.embedding <=> query_embedding
    LIMIT match_count;
  END IF;
END;
$$;

-- Hybrid search function
CREATE OR REPLACE FUNCTION hybrid_search(
  query_text TEXT,
  query_embedding vector(1536),
  search_table TEXT DEFAULT 'document_chunks',
  text_weight float DEFAULT 0.3,
  vector_weight float DEFAULT 0.7,
  match_count int DEFAULT 10
)
RETURNS TABLE (
  id UUID,
  content TEXT,
  rank float,
  metadata JSONB
)
LANGUAGE plpgsql STABLE
AS $$
BEGIN
  IF search_table = 'document_chunks' THEN
    RETURN QUERY
    WITH text_search AS (
      SELECT
        dc.id,
        ts_rank_cd(to_tsvector('english', dc.content), plainto_tsquery('english', query_text)) as text_rank
      FROM document_chunks dc
      WHERE to_tsvector('english', dc.content) @@ plainto_tsquery('english', query_text)
    ),
    vector_search AS (
      SELECT
        dc.id,
        1 - (dc.embedding <=> query_embedding) as vector_rank
      FROM document_chunks dc
    )
    SELECT
      v.id,
      dc.content,
      (COALESCE(t.text_rank, 0) * text_weight + v.vector_rank * vector_weight) as rank,
      dc.chunk_metadata as metadata
    FROM vector_search v
    LEFT JOIN text_search t ON v.id = t.id
    JOIN document_chunks dc ON v.id = dc.id
    ORDER BY rank DESC
    LIMIT match_count;
  END IF;
END;
$$;
EOF

# Apply functions migration
docker exec -i $(docker-compose ps -q postgres) psql -U postgres -d mastra_bi < migrations/002-create-functions.sql
```

## Phase 2: Enhanced LangFuse Observability

### 2.1 Install LangFuse Dependencies

```bash
# Install LangFuse SDK and observability tools
pnpm add langfuse
```

### 2.2 LangFuse Configuration

```bash
# Add LangFuse configuration to .env
cat << 'EOF' >> .env
# LangFuse Observability (Constitutional Requirement)
LANGFUSE_PUBLIC_KEY=pk_lf_your_public_key
LANGFUSE_SECRET_KEY=sk_lf_your_secret_key
LANGFUSE_BASE_URL=https://cloud.langfuse.com
LANGFUSE_ENABLED=true
EOF
```

### 2.3 Create Enhanced Observability Implementation

```bash
# Create observability directory structure
mkdir -p src/mastra/observability/enhanced

# Create comprehensive tracing implementation
cat << 'EOF' > src/mastra/observability/enhanced/comprehensive-tracer.ts
import { Langfuse } from 'langfuse';

// Initialize LangFuse client with circuit breaker
const langfuse = new Langfuse({
  publicKey: process.env.LANGFUSE_PUBLIC_KEY!,
  secretKey: process.env.LANGFUSE_SECRET_KEY!,
  baseUrl: process.env.LANGFUSE_BASE_URL,
});

export class ComprehensiveTracer {
  private circuitBreaker = new LangFuseCircuitBreaker();

  async createTrace(name: string, options: {
    userId?: string;
    sessionId?: string;
    input?: any;
    metadata?: Record<string, any>;
  }) {
    return await this.circuitBreaker.execute(async () => {
      return langfuse.trace({
        name,
        userId: options.userId,
        sessionId: options.sessionId,
        input: options.input,
        metadata: options.metadata,
      });
    });
  }

  async recordToolCall(
    traceId: string,
    toolName: string,
    input: any,
    output: any,
    metadata: {
      duration: number;
      serverName?: string;
      error?: string;
    }
  ) {
    return await this.circuitBreaker.execute(async () => {
      const trace = langfuse.trace({ id: traceId });
      return trace?.event({
        name: `Tool Call: ${toolName}`,
        level: metadata.error ? 'ERROR' : 'DEFAULT',
        input,
        output,
        metadata: {
          toolName,
          serverName: metadata.serverName,
          duration: metadata.duration,
          error: metadata.error,
          timestamp: new Date().toISOString(),
        },
      });
    });
  }
}

class LangFuseCircuitBreaker {
  private failures = 0;
  private lastFailTime = 0;
  private state: 'closed' | 'open' | 'half-open' = 'closed';

  private readonly failureThreshold = 5;
  private readonly recoveryTimeout = 30000;

  async execute<T>(operation: () => Promise<T>): Promise<T | null> {
    if (this.state === 'open') {
      if (Date.now() - this.lastFailTime >= this.recoveryTimeout) {
        this.state = 'half-open';
        this.failures = 0;
      } else {
        console.warn('LangFuse circuit breaker is open - skipping operation');
        return null;
      }
    }

    try {
      const result = await operation();

      if (this.state === 'half-open') {
        this.state = 'closed';
        this.failures = 0;
      }

      return result;
    } catch (error) {
      this.failures++;
      this.lastFailTime = Date.now();

      if (this.failures >= this.failureThreshold) {
        this.state = 'open';
        console.error(`LangFuse circuit breaker opened after ${this.failures} failures`);
      }

      console.warn('LangFuse operation failed:', error);
      return null;
    }
  }
}

export const comprehensiveTracer = new ComprehensiveTracer();
EOF
```

## Phase 3: Supabase MCP Server Integration

### 3.1 Install MCP Dependencies

```bash
# Install Supabase MCP server and dependencies
pnpm add @supabase/mcp-server-supabase @modelcontextprotocol/sdk
```

### 3.2 Configure Supabase MCP Server

```bash
# Create MCP configuration
cat << 'EOF' > mcp.json
{
  "mcpServers": {
    "supabase": {
      "command": "npx",
      "args": [
        "-y",
        "@supabase/mcp-server-supabase@latest",
        "--features=database,docs",
        "--project-ref=your-project-ref"
      ],
      "env": {
        "SUPABASE_ACCESS_TOKEN": "your-personal-access-token"
      }
    },
    "mastra": {
      "command": "npx",
      "args": [
        "-y",
        "@mastra/mcp-server@latest"
      ]
    },
    "context7": {
      "command": "npx",
      "args": [
        "-y",
        "@context7/mcp-server@latest"
      ]
    }
  }
}
EOF

# Add MCP configuration to environment
cat << 'EOF' >> .env
# MCP Configuration (Constitutional Requirement)
MCP_CONFIG_PATH=./mcp.json
SUPABASE_PROJECT_REF=your_project_ref
SUPABASE_ACCESS_TOKEN=your_personal_access_token
EOF
```

### 3.3 Create MCP Integration Service

```bash
# Create MCP integration directory
mkdir -p src/mastra/mcp/integration

# Create MCP client implementation
cat << 'EOF' > src/mastra/mcp/integration/client.ts
import { Client } from '@modelcontextprotocol/sdk/client/index.js';
import { comprehensiveTracer } from '../../observability/enhanced/comprehensive-tracer.js';

export class MCPIntegrationClient {
  private clients: Map<string, Client> = new Map();

  async connectToServer(serverName: string, config: MCPServerConfig): Promise<boolean> {
    try {
      const client = new Client(
        { name: 'Mastra-BI-System', version: '1.0.0' },
        { capabilities: { tools: {} } }
      );

      // Configure transport based on server type
      const transport = this.createTransport(config);
      await client.connect(transport);

      this.clients.set(serverName, client);
      console.log(`Connected to MCP server: ${serverName}`);
      return true;
    } catch (error) {
      console.error(`Failed to connect to MCP server ${serverName}:`, error);
      return false;
    }
  }

  async callTool(
    serverName: string,
    toolName: string,
    args: any,
    userContext?: { userId?: string; traceId?: string }
  ): Promise<any> {
    const client = this.clients.get(serverName);
    if (!client) {
      throw new Error(`MCP server ${serverName} not connected`);
    }

    const startTime = Date.now();
    let result, error;

    try {
      result = await client.callTool({ name: toolName, arguments: args });
    } catch (err) {
      error = err;
      throw err;
    } finally {
      // Record tool call in LangFuse
      if (userContext?.traceId) {
        await comprehensiveTracer.recordToolCall(
          userContext.traceId,
          toolName,
          args,
          result,
          {
            duration: Date.now() - startTime,
            serverName,
            error: error ? String(error) : undefined,
          }
        );
      }
    }

    return result;
  }

  private createTransport(config: MCPServerConfig) {
    // Implementation details for transport creation
    // This would create appropriate transport based on config.type
    throw new Error('Transport creation not implemented in quickstart');
  }
}

interface MCPServerConfig {
  type: 'stdio' | 'http' | 'sse';
  command?: string;
  args?: string[];
  url?: string;
}

export const mcpClient = new MCPIntegrationClient();
EOF
```

## Phase 4: API Validation Implementation

### 4.1 Create Validation Service

```bash
# Create validation directory
mkdir -p src/mastra/validation

# Create API validation service
cat << 'EOF' > src/mastra/validation/framework-validator.ts
import { mcpClient } from '../mcp/integration/client.js';

export class FrameworkValidator {
  async validateMastraCompliance(components: any[]): Promise<ValidationResult> {
    try {
      // Validate against Mastra MCP server
      const mastraValidation = await mcpClient.callTool(
        'mastra',
        'validate_components',
        { components }
      );

      return {
        server: 'mastra',
        compliant: mastraValidation.compliant,
        issues: mastraValidation.issues || [],
        recommendations: mastraValidation.recommendations || [],
      };
    } catch (error) {
      console.error('Mastra validation failed:', error);
      return {
        server: 'mastra',
        compliant: false,
        issues: [{ type: 'validation_error', message: String(error) }],
        recommendations: ['Check Mastra MCP server connectivity'],
      };
    }
  }

  async validateContext7Compliance(library: string, usage: any): Promise<ValidationResult> {
    try {
      // Validate against Context7 MCP server
      const context7Validation = await mcpClient.callTool(
        'context7',
        'validate_usage',
        { library, usage }
      );

      return {
        server: 'context7',
        compliant: context7Validation.compliant,
        issues: context7Validation.issues || [],
        recommendations: context7Validation.recommendations || [],
      };
    } catch (error) {
      console.error('Context7 validation failed:', error);
      return {
        server: 'context7',
        compliant: false,
        issues: [{ type: 'validation_error', message: String(error) }],
        recommendations: ['Check Context7 MCP server connectivity'],
      };
    }
  }

  async validateFullCompliance(): Promise<ComplianceReport> {
    // Validate against both servers for constitutional compliance
    const mastraResult = await this.validateMastraCompliance([]);
    const context7Result = await this.validateContext7Compliance('mastra', {});

    return {
      overall: mastraResult.compliant && context7Result.compliant,
      mastraCompliance: mastraResult,
      context7Compliance: context7Result,
      timestamp: new Date().toISOString(),
    };
  }
}

interface ValidationResult {
  server: 'mastra' | 'context7';
  compliant: boolean;
  issues: Array<{ type: string; message: string }>;
  recommendations: string[];
}

interface ComplianceReport {
  overall: boolean;
  mastraCompliance: ValidationResult;
  context7Compliance: ValidationResult;
  timestamp: string;
}

export const frameworkValidator = new FrameworkValidator();
EOF
```

## Phase 5: Integration and Testing

### 5.1 Update Main Mastra Configuration

```bash
# Update src/mastra/index.ts to remove Supabase database dependencies
# and add constitutional compliance components

# Create backup of current implementation
cp src/mastra/index.ts src/mastra/index.ts.backup

# Update with constitutional compliance
cat << 'EOF' > src/mastra/config/database.ts
import { drizzle } from 'drizzle-orm/node-postgres';
import { Client } from 'pg';
import * as schema from '../database/schema.js';

// pgvector 17 connection (Constitutional Requirement)
const client = new Client({
  connectionString: process.env.PGVECTOR_DATABASE_URL!,
});

await client.connect();

export const db = drizzle(client, { schema });
export { client as pgClient };
EOF
```

### 5.2 Validation Tests

```bash
# Create validation test script
cat << 'EOF' > scripts/validate-compliance.js
#!/usr/bin/env node

import { frameworkValidator } from '../src/mastra/validation/framework-validator.js';
import { comprehensiveTracer } from '../src/mastra/observability/enhanced/comprehensive-tracer.js';
import { db } from '../src/mastra/config/database.js';

async function validateConstitutionalCompliance() {
  console.log('üîç Validating Constitutional Compliance...\n');

  // Test 1: pgvector Database Architecture
  console.log('‚úÖ Testing pgvector 17 database connection...');
  try {
    const result = await db.execute('SELECT version(), version() AS pgvector_version');
    console.log('   ‚úÖ pgvector 17 database connected successfully');
  } catch (error) {
    console.error('   ‚ùå pgvector database connection failed:', error.message);
    process.exit(1);
  }

  // Test 2: Postgres Functions
  console.log('‚úÖ Testing postgres functions...');
  try {
    const testEmbedding = Array(1536).fill(0.1);
    const result = await db.execute(`
      SELECT * FROM semantic_search($1::vector, 'user_memories', 'test_user', 0.5, 1)
    `, [JSON.stringify(testEmbedding)]);
    console.log('   ‚úÖ Postgres functions working correctly');
  } catch (error) {
    console.error('   ‚ùå Postgres functions test failed:', error.message);
  }

  // Test 3: LangFuse Observability
  console.log('‚úÖ Testing LangFuse observability...');
  try {
    const trace = await comprehensiveTracer.createTrace('compliance-test', {
      userId: 'test-user',
      metadata: { test: 'constitutional-compliance' }
    });
    console.log('   ‚úÖ LangFuse tracing working correctly');
  } catch (error) {
    console.error('   ‚ùå LangFuse observability test failed:', error.message);
  }

  // Test 4: Framework Validation
  console.log('‚úÖ Testing API validation...');
  try {
    const complianceReport = await frameworkValidator.validateFullCompliance();
    console.log('   ‚úÖ Framework validation completed');
    console.log('   üìä Overall compliance:', complianceReport.overall);
  } catch (error) {
    console.error('   ‚ùå Framework validation test failed:', error.message);
  }

  console.log('\nüéâ Constitutional compliance validation completed!');
}

validateConstitutionalCompliance().catch(console.error);
EOF

chmod +x scripts/validate-compliance.js
```

### 5.3 Run Validation

```bash
# Run constitutional compliance validation
node scripts/validate-compliance.js
```

## Phase 6: Deployment Verification

### 6.1 Health Check Implementation

```bash
# Create health check endpoint
cat << 'EOF' > src/mastra/api/health/constitutional-health.ts
import { Request, Response } from 'express';
import { db } from '../../config/database.js';
import { comprehensiveTracer } from '../../observability/enhanced/comprehensive-tracer.js';
import { mcpClient } from '../../mcp/integration/client.js';

export async function checkConstitutionalHealth(req: Request, res: Response) {
  const healthChecks = {
    pgvectorDatabase: false,
    postgresFunction: false,
    langfuseObservability: false,
    supabaseMcpServer: false,
    overall: false,
  };

  try {
    // Check pgvector database
    await db.execute('SELECT 1');
    healthChecks.pgvectorDatabase = true;

    // Check postgres functions
    const testEmbedding = Array(1536).fill(0.1);
    await db.execute(`
      SELECT * FROM semantic_search($1::vector, 'user_memories', 'health-check', 0.5, 1)
    `, [JSON.stringify(testEmbedding)]);
    healthChecks.postgresFunction = true;

    // Check LangFuse
    const trace = await comprehensiveTracer.createTrace('health-check', {
      metadata: { type: 'constitutional-health' }
    });
    healthChecks.langfuseObservability = !!trace;

    // Check Supabase MCP server
    try {
      await mcpClient.callTool('supabase', 'list_projects', {});
      healthChecks.supabaseMcpServer = true;
    } catch (error) {
      console.warn('Supabase MCP server check failed:', error.message);
    }

    healthChecks.overall = Object.values(healthChecks).every(check => check === true);

    res.status(healthChecks.overall ? 200 : 503).json({
      status: healthChecks.overall ? 'healthy' : 'unhealthy',
      constitutionalCompliance: healthChecks,
      timestamp: new Date().toISOString(),
    });

  } catch (error) {
    res.status(503).json({
      status: 'unhealthy',
      error: error.message,
      constitutionalCompliance: healthChecks,
      timestamp: new Date().toISOString(),
    });
  }
}
EOF
```

### 6.2 Final Verification

```bash
# Start the application
pnpm dev

# Test constitutional compliance endpoint
curl http://localhost:3000/api/health/constitutional

# Expected response should show all checks passing:
# {
#   "status": "healthy",
#   "constitutionalCompliance": {
#     "pgvectorDatabase": true,
#     "postgresFunction": true,
#     "langfuseObservability": true,
#     "supabaseMcpServer": true,
#     "overall": true
#   }
# }
```

## Success Criteria Verification

After completing this quickstart, verify all constitutional requirements are met:

- ‚úÖ **pgvector 17 Database Architecture**: Database uses pgvector 17 with postgres functions via Drizzle ORM
- ‚úÖ **Comprehensive LangFuse Observability**: All tool calls, agent interactions, and workflow executions are traced
- ‚úÖ **Supabase MCP Server Integration**: Built-in Supabase MCP server is configured and functional
- ‚úÖ **Mastra API Validation**: Framework compliance is validated against Mastra and Context7 MCP servers
- ‚úÖ **Feature-Based Clean Architecture**: All changes maintain existing organizational patterns

## Troubleshooting

### Common Issues

**Database Connection Issues:**
```bash
# Check Docker container status
docker-compose ps
docker-compose logs postgres

# Verify pgvector extension
docker exec -it $(docker-compose ps -q postgres) psql -U postgres -d mastra_bi -c "SELECT * FROM pg_extension WHERE extname = 'vector';"
```

**MCP Server Connection Issues:**
```bash
# Test MCP server connectivity manually
npx -y @supabase/mcp-server-supabase@latest --help

# Check MCP configuration
cat mcp.json
```

**LangFuse Observability Issues:**
```bash
# Verify environment variables
echo $LANGFUSE_PUBLIC_KEY
echo $LANGFUSE_SECRET_KEY

# Test LangFuse connectivity
curl -H "Authorization: Bearer $LANGFUSE_PUBLIC_KEY" https://cloud.langfuse.com/api/public/health
```

This quickstart ensures complete constitutional compliance while maintaining system functionality and performance.